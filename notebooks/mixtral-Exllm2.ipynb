{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n",
    "from peft import LoraConfig, PeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\n",
    "import os, torch, wandb, platform, warnings\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"/notebooks/models/parser/models/models/MixtralExLLm/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exllamav2 import(\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Config,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Tokenizer,\n",
    ")\n",
    "\n",
    "from exllamav2.generator import (\n",
    "    ExLlamaV2BaseGenerator,\n",
    "    ExLlamaV2Sampler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ExLlamaV2Config()\n",
    "config.model_dir = base_model\n",
    "config.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /notebooks/models/parser/models/models/MixtralExLLm/\n"
     ]
    }
   ],
   "source": [
    "model = ExLlamaV2(config)\n",
    "print(\"Loading model: \" + base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//remove cache if memory fke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = ExLlamaV2Cache(model, lazy = True)\n",
    "model.load_autosplit(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ExLlamaV2Tokenizer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ExLlamaV2BaseGenerator(model, cache, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = ExLlamaV2Sampler.Settings()\n",
    "settings.temperature = 0.5\n",
    "settings.top_k = 40\n",
    "settings.top_p = 0.9\n",
    "settings.token_repetition_penalty = 1.1\n",
    "settings.typical = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a helpful AI assistant, you are an agent capable of using a variety of tools to answer a question. Here are a few of the tools available to you:\n",
    "\n",
    "- Calculator: the calculator should be used whenever you need to perform a calculation, no matter how simple. It uses Python so make sure to write complete Python code required to perform the calculation required and make sure the Python returns your answer to the `output` variable.\n",
    "- Search: the search tool should be used whenever you need to find information. It can be used to find information about everything\n",
    "- Final Answer: the final answer tool must be used to respond to the user. You must use this when you have decided on an answer.\n",
    "\n",
    "To use these tools you must always respond in JSON format containing `\"tool_name\"` and `\"input\"` key-value pairs. For example, to answer the question, \"what is the square root of 51?\" you must use the calculator tool like so:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"tool_name\": \"Calculator\",\n",
    "    \"input\": \"from math import sqrt; output = sqrt(51)\"\n",
    "}\n",
    "```\n",
    "\n",
    "Or to answer the question \"who is the current president of the USA?\" you must respond:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"tool_name\": \"Search\",\n",
    "    \"input\": \"current president of USA\"\n",
    "}\n",
    "```\n",
    "\n",
    "Remember, even when answering to the user, you must still use this JSON format! If you'd like to ask how the user is doing you must write:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"tool_name\": \"Final Answer\",\n",
    "    \"input\": \"How are you today?\"\n",
    "}\n",
    "```\n",
    "\n",
    "Let's get started. The users query is as follows.\n",
    "\n",
    "User: Is there a new harry potter movie coming out?\n",
    "\n",
    "Assistant: ```json\n",
    "{\n",
    "    \"tool_name\": \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful AI assistant, you are an agent capable of using a variety of tools to answer a question. Here are a few of the tools available to you:\n",
      "\n",
      "- Calculator: the calculator should be used whenever you need to perform a calculation, no matter how simple. It uses Python so make sure to write complete Python code required to perform the calculation required and make sure the Python returns your answer to the `output` variable.\n",
      "- Search: the search tool should be used whenever you need to find information. It can be used to find information about everything\n",
      "- Final Answer: the final answer tool must be used to respond to the user. You must use this when you have decided on an answer.\n",
      "\n",
      "To use these tools you must always respond in JSON format containing `\"tool_name\"` and `\"input\"` key-value pairs. For example, to answer the question, \"what is the square root of 51?\" you must use the calculator tool like so:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"tool_name\": \"Calculator\",\n",
      "    \"input\": \"from math import sqrt; output = sqrt(51)\"\n",
      "}\n",
      "```\n",
      "\n",
      "Or to answer the question \"who is the current president of the USA?\" you must respond:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"tool_name\": \"Search\",\n",
      "    \"input\": \"current president of USA\"\n",
      "}\n",
      "```\n",
      "\n",
      "Remember, even when answering to the user, you must still use this JSON format! If you'd like to ask how the user is doing you must write:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"tool_name\": \"Final Answer\",\n",
      "    \"input\": \"How are you today?\"\n",
      "}\n",
      "```\n",
      "\n",
      "Let's get started. The users query is as follows.\n",
      "\n",
      "User: Is there a new harry potter movie coming out?\n",
      "\n",
      "Assistant: ```json\n",
      "{\n",
      "    \"tool_name\":  \"Search\",\n",
      "    \"input\":  \"new harry potter movie release date\"\n",
      "}\n",
      "```\n",
      "\n",
      "Response generated in 0.99 seconds, 512 tokens, 516.79 tokens/second\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Our story begins in the Scottish town of Auchtermuchty, where once\"\n",
    "\n",
    "max_new_tokens = 512\n",
    "\n",
    "generator.warmup()\n",
    "time_begin = time.time()\n",
    "\n",
    "output = generator.generate_simple(prompt, settings, max_new_tokens, seed = 1234)\n",
    "\n",
    "time_end = time.time()\n",
    "time_total = time_end - time_begin\n",
    "\n",
    "print(output)\n",
    "print()\n",
    "print(f\"Response generated in {time_total:.2f} seconds, {max_new_tokens} tokens, {max_new_tokens / time_total:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [12419]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     66.96.207.228:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     66.96.207.228:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import uvicorn\n",
    "import fastapi\n",
    "\n",
    "app = fastapi.FastAPI()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = uvicorn.Config(app)\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_running_loop()\n",
    "    loop.create_task(server.serve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
