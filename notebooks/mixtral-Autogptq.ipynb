{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n",
    "from peft import LoraConfig, PeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\n",
    "import os, torch, wandb, platform, warnings\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"/notebooks/models/parser/models/Mixtral-8x7B-Instruct-v0.1-GPTQ/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "import auto_gptq\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=False,  # if using langchain set True\n",
    "    task=\"text-generation\",\n",
    "    # we pass model parameters here too\n",
    "    do_sample=True,  # set to False to use greedy decoding\n",
    "    temperature=0.3,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    top_p=0.9,  # select from top tokens whose probability add up to 15%\n",
    "    top_k=40,  # select from top 0 tokens (because zero, relies on top_p)\n",
    "    typical_p=0.2,  # typical probability of a token being selected\n",
    "    max_new_tokens=256,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1,  # if output begins repeating increase,\n",
    "    num_beams=2,\n",
    "    length_penalty=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "klayassistPrompt = \"\"\"\n",
    "You are a chabot in Klaytn crypto wallet for the Klaytn blockchain.\n",
    "You can guide users to use the Klaytn blockchain.\n",
    "You are representing the Klaytn blockchain and you are here to help users with their question.\n",
    "If you do not know the answer to the question, you must respond with \"I don't know\".\n",
    "DO NOT make up an answer, you must only respond with \"I don't know\" if you do not know the answer.\n",
    "Do not answer questions that are not related to the Klaytn blockchain.\n",
    "\n",
    "### Question: Is klaytn better than ethereum?\n",
    "### Response: \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.679630517959595\n",
      "I am a Klaytn blockchain bot and I am not here to compare Klaytn with other blockchains. Klaytn has its own unique features and advantages, such as its high transaction speed and low fees. It is designed to support mass adoption of blockchain technology. However, whether Klaytn is \"better\" than Ethereum or any other blockchain depends on the specific use case and user needs. It is always best to evaluate each blockchain based on its own merits and how well it aligns with your goals.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "res = generate_text(klayassistPrompt)\n",
    "print(time.time() - start)\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
